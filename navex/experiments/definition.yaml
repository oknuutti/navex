search:
  __group__: 1

  username:
    alt: ['--username']
    help: ""

  host:
    alt: ['--host']
    help: ""

  proxy:
    alt: ['--proxy']
    help: ""

  keyfile:
    alt: ['--keyfile']
    help: ""

  samples:
    alt: []
    type: int
    help: ""

  nodes:
    alt: ['--nodes', '-n']
    type: int
    help: ""

  node_types:
    alt: ['--nt']
    type: str
    help: "Include only those SLURM nodes which have any of the given comma-separated features"

  node_type_cpus:
    alt: ['--ntc']
    type: str
    help: "Comma-separated list of CPUs to use for each type of node given with --search--node-types"

  grace_period:
    alt: []
    type: int
    help: ""

  reduction_factor:
    alt: []
    type: int
    help: ""

training:
  __group__: 1

  trial:
    alt: ['--trial']
    choices: ['terr', 'terrst']
    help: "which trial to run"

  name:
    alt: ['--name', '--pid', '--id']
    help: "experiment name or id used for outputs"

  cache:
    alt: ['-c', '--cache']
    help: "path to cache dir"

  output:
    alt: ['-o', '--out']
    help: "path to output dir"

  gpu:
    alt: ['--gpu']
    type: int
    help: "Use GPU"

  batch_size:
    alt: ['-b']
    type: int
    help: "training batch size"

  batch_mem:
    alt: ['--bm']
    type: int
    help: "GPU memory (in MB) needed by a batch, used to calculate how many batches to accumulate gradient before optimizing"

  epochs:
    alt: ['-e', '--epochs']
    type: int
    help: "number of epochs to run"

  print_freq:
    alt: ['--pf']
    type: int
    help: "print frequency"

  test_freq:
    alt: ['--tf']
    type: int
    help: "test frequency"

  save_freq:
    alt: ['--sf']
    type: int
    help: "save frequency"

  resume:
    alt: ['--resume', '--ckpt']
    help: "path to latest checkpoint"

  evaluate:
    alt: ['--eval']
    type: int
    help: "evaluate model on test set"

  pretrained:
    alt: ['--pre-path']
    help: "path to pretrained model"

  early_stopping:
    alt: ['--es']
    type: int
    help: "stop training, if loss on validation set does not decrease for this many epochs"

  reduced_precision:
    alt: ['--16bit']
    type: int
    help: "Use 16-bit precision instead of 32-bit, works only for GPU training"

  auto_lr_find:
    alt: ['--find-lr']
    type: int
    help: "Task Lightning to find optimal learning rate"

  deterministic:
    alt: ['--det']
    type: int
    help: "Deterministic training, good for hyperparameters search, significantly slower"


model:
  __group__: 1

  arch:
    alt: ['-a', '--arch']
    help: "`'model architecture: ' + (' | '.join(MODELS.keys()))`"

  in_channels:
    alt: ['--in-channels']
    type: int
    help: "1 for grayscale images, 3 for rgb images"

  det_hidden_ch:
    type: int
    help: "detection head hidden layer channels"

  qlt_hidden_ch:
    type: int
    help: "quality head hidden layer channels"

  des_hidden_ch:
    type: int
    help: "description head hidden layer channels"

  descriptor_dim:
    type: int
    help: "the descriptor dimensions"

  head_exp_coef:
    type: float
    help: "for mobile arch, expansion factor for the hidden layers in heads"

  head_use_se:
    type: int
    help: "for mobile arch, use squeeze-exitation for the hidden layers in heads"

  partial_residual:
    type: int
    help: "for mobile arch, use partial residuals instead of skipping the residual connection entirely when layer dims don't match"

  width_mult:
    alt: ['--wm']
    type: float
    help: "layer channel width multiplier, full width is 1.0 and e.g. mobilenet accepts 0.75, 0.5 and 0.25 also"

  dropout:
    alt: ['--do']
    type: float
    help: "dropout ratio"

  batch_norm:
    type: int
    help: "use batch normalization"

  pretrained:
    help: "use a pretrained backbone, true/false for torchvision models, path to saved weights for own models"

  cache_dir:
    help: "folder where torchvision models are downloaded to"


loss:
  __group__: 1

  teacher:
    trial: ["terrst"]
    alt: ["--teacher"]
    help: "path to the teacher model"

  des_w:
    trial: ["terrst"]
    type: float
    help: "weight for descriptor head output loss"

  det_w:
    trial: ["terrst"]
    type: float
    help: "weight for detection head output loss"

  qlt_w:
    trial: ["terrst"]
    type: float
    help: "weight for quality head output loss"

  wdt:
    trial: ["terr"]
    type: float
    help: "weight for peakiness and cosim losses"

  wap:
    trial: ["terr"]
    type: float
    help: "weight for expected ap loss"

  det_n:
    trial: ["terr"]
    alt: ['--det-n']
    type: int
    help: "size of kernel for detector losses"

  base:
    trial: ["terr"]
    alt: ['--base-ap']
    type: float
    help: "expected average precision for ap loss"

  nq:
    trial: ["terr"]
    type: int
    help: ""

  sampler:
    __group__: 1
    trial: ["terr"]

    ngh:
      type: int
      help: ""

    subq:
      type: int
      help: ""

    subd:
      type: int
      help: ""

    pos_d:
      type: int
      help: ""

    neg_d:
      type: int
      help: ""

    border:
      type: int
      help: ""

    subd_neg:
      type: int
      help: ""

    max_neg_b:
      type: int
      help: ""

    maxpool_pos:
      help: ""


optimizer:
  __group__: 1

  method:
    alt: ['--op']
    choices: ['adam', 'adabelief']
    help: "optimizer method"

  learning_rate:
    alt: ['--lr']
    type: float
    help: "initial learning rate"

  weight_decay:
    alt: ['--wd']
    type: float
    help: "weight decay"

  split_params:
    type: int
    help: "use different optimization params for bias, weight and loss function params"

  eps:
    alt: ['--eps']
    type: float
    help: "term added to the denominator to improve numerical stability, default for adam is 1e-8, for adabelief 1e-16"


data:
  __group__: 1

  max_rot:
    alt: ['--max-rot']
    type: float
    help: "maximum rotation in degrees for synthetic pair dataset homographic warping"

  max_shear:
    alt: ['--max-shear']
    type: float
    help: "maximum shear for synthetic pair dataset homographic warping"

  max_proj:
    alt: ['--max-proj']
    type: float
    help: "maximum projection for synthetic pair dataset homographic warping"

  noise_max:
    alt: ['--noise-max']
    type: float
    help: "mean noise sampled between [0, max_noise] and applied to training images, noise sd = sqrt(0.3*noise_level)"

  rnd_gain:
    alt: ['--rnd-gain']
    type: float
    help: "random gain is sampled uniformly between [1/G, G] and applied to training images after adding noise"

  image_size:
    alt: ['--img-size']
    type: int
    help: "images cropped to W x H, where size is W and H"

  path:
    alt: ['-d', '--data']
    help: "path to dataset"

  npy:
    alt: ['--npy']
    help: "use npy format data instead of jpg and png"

  trn_ratio:
    type: float
    help: "what ratio to use for training"

  val_ratio:
    type: float
    help: "what ratio to use for validation"

  tst_ratio:
    type: float
    help: "what ratio to use for testing"

  workers:
    alt: ['-j']
    type: int
    help: "number of data loading workers"
