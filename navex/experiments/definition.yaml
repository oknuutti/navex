search:
  __group__: 1

  username:
    alt: ['--username']
    help: ""

  host:
    alt: ['--host']
    help: ""

  proxy:
    alt: ['--proxy']
    help: ""

  keyfile:
    alt: ['--keyfile']
    help: ""

  samples:
    alt: []
    type: int
    help: ""

  workers:
    alt: []
    type: int
    help: ""

  grace_period:
    alt: []
    type: int
    help: ""

  reduction_factor:
    alt: []
    type: int
    help: ""

training:
  __group__: 1

  name:
    alt: ['--name', '--pid', '--id']
    help: "experiment name or id used for outputs"

  cache:
    alt: ['-c', '--cache']
    help: "path to cache dir"

  output:
    alt: ['-o', '--out']
    help: "path to output dir"

  gpu:
    alt: ['--gpu']
    type: int
    help: "Use GPU"

  batch_size:
    alt: ['-b']
    type: int
    help: "training batch size"

  acc_grad_batches:
    alt: ['--bx']
    type: int
    help: "accumulate gradient this many batches before optimizing"

  epochs:
    alt: ['-e', '--epochs']
    type: int
    help: "number of epochs to run"

  print_freq:
    alt: ['--pf']
    type: int
    help: "print frequency"

  test_freq:
    alt: ['--tf']
    type: int
    help: "test frequency"

  save_freq:
    alt: ['--sf']
    type: int
    help: "save frequency"

  resume:
    alt: ['--resume']
    help: "path to latest checkpoint"

  evaluate:
    alt: ['--eval']
    type: int
    help: "evaluate model on test set"

  pretrained:
    alt: ['--pre-path']
    help: "path to pretrained model"

  early_stopping:
    alt: ['--es']
    type: int
    help: "stop training, if loss on validation set does not decrease for this many epochs"

  reduced_precision:
    alt: ['--16bit']
    type: int
    help: "Use 16-bit precision instead of 32-bit, works only for GPU training"

  auto_lr_find:
    alt: ['--find-lr']
    type: int
    help: "Task Lightning to find optimal learning rate"

  deterministic:
    alt: ['--det']
    type: int
    help: "Deterministic training, good for hyperparameters search, significantly slower"


model:
  __group__: 1

  arch:
    alt: ['-a', '--arch']
    help: "`'model architecture: ' + (' | '.join(MODELS.keys()))`"

  head_conv_ch:
    type: int
    help: "channels in extra feature layer before heads"

  descriptor_dim:
    type: int
    help: "the descriptor dimensions"

  width_mult:
    alt: ['--wm']
    type: float
    help: "layer channel width multiplier, full width is 1.0 and e.g. mobilenet accepts 0.75, 0.5 and 0.25 also"

  dropout:
    alt: ['--do']
    type: float
    help: "dropout ratio"

  excl_bn_affine:
    type: int
    help: "do not use affine transformation after batch normalization"

  batch_norm:
    type: int
    help: "use batch normalization"

  direct_detection:
    type: int
    help: "do not use multiple channels for the detector head"

  pretrained:
    help: "use a pretrained backbone, true/false for torchvision models, path to saved weights for own models"

  cache_dir:
    help: "folder where torchvision models are downloaded to"


loss:
  __group__: 1

  wp:
    type: float
    help: "peakiness loss weight"

  wc:
    type: float
    help: "cosim loss weight"

  wa:
    type: float
    help: "average precision loss weight"

  det_n:
    type: int
    help: "size of kernel for detector losses"

  base:
    type: float
    help: "expected average precision for ap loss"

  nq:
    type: int
    help: ""

  sampler:
    __group__: 1

    ngh:
      type: int
      help: ""

    subq:
      type: int
      help: ""

    subd:
      type: int
      help: ""

    pos_d:
      type: int
      help: ""

    neg_d:
      type: int
      help: ""

    border:
      type: int
      help: ""

    subd_neg:
      type: int
      help: ""

    max_neg_b:
      type: int
      help: ""

    maxpool_pos:
      help: ""


optimizer:
  __group__: 1

  method:
    alt: ['--op']
    help: "optimizer method, only 'adam' is currently supported"

  learning_rate:
    alt: ['--lr']
    type: float
    help: "initial learning rate"

  weight_decay:
    alt: ['--wd']
    type: float
    help: "weight decay"

  split_params:
    type: int
    help: "use different optimization params for bias, weight and loss function params"

  excl_bn:
    type: int
    help: "exclude batch norm params from optimization"

  eps:
    type: float
    help: "term added to the denominator to improve numerical stability"


data:
  __group__: 1

  path:
    alt: ['-d', '--data']
    help: "path to dataset"

  trn_ratio:
    type: float
    help: "what ratio to use for training"

  val_ratio:
    type: float
    help: "what ratio to use for validation"

  tst_ratio:
    type: float
    help: "what ratio to use for testing"

  workers:
    alt: ['-j']
    type: int
    help: "number of data loading workers"
