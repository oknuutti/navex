search:
  method: 'asha-bo'
  resume: ''
  metric: 'loss'
  mode: 'min'
  samples: 243
  nodes: 6
  node_types: 'volta'   # volta for triton, type_gpu for csc

  # doesnt really work to have different cpu counts as tune.run(..., resources_per_trial={"cpu":...}) needs to be set
  node_type_cpus: '6'
  grace_period: 1
  reduction_factor: 3
  username: ''
  keyfile: ''
  proxy: ''
  host: ''
  port: 0

training:
  trial: 'terrst'
  name: 'v0'
  cache: 'cache'
  output: 'output'
  batch_size: 32
  batch_mem: 32000
  gpu: 1
  epochs: 20
  print_freq: 5
  test_freq: 1
  save_freq: 1
  resume: null
  evaluate: 0
  pretrained: ''
  early_stopping: 6
  reduced_precision: 0
  auto_lr_find: 0
  deterministic: 1

model:
  arch:             !h_tune2_choice  '["mob-mn3_L"], ["mob-mn2_L", "mob-mn3_L", "mob-en_0"]'
  in_channels:      1
  partial_residual: 0
  width_mult:       !h_tune2_choice  '[1.0], [0.75, 1.0, 1.25, 1.5]'
  pretrained: 0         # not in use
  cache_dir: 'cache'    # not in use
  des_head:
    dimensions:  128
    hidden_ch:   !h_tune2_choice     '[128], [96, 128, 160]'
    exp_coef:    !h_tune2_choice     '[7.5], [0, 5.6, 7.5, 10.0, 13.3]'
    use_se:      !h_tune2_choice     '[0], [0, 1]'
    dropout:     0.0
  det_head:
    after_des:   !h_tune2_choice     '[1], [0, 1]'
    hidden_ch:   !h_tune2_choice     '[0], [0, 64, 128]'
    exp_coef:    !h_tune2_choice     '[7.5], [0, 5.6, 7.5, 10.0, 13.3]'
    use_se:      !h_tune2_choice     '[0], [0, 1]'
    dropout:     0.0
  qlt_head:
    skip:        !h_tune2_choice     '[0], [0, 1]'
    after_des:   !h_tune2_choice     '[1], [0, 1]'
    hidden_ch:   !h_tune2_choice     '[0], [0, 64, 128]'
    exp_coef:    !h_tune2_choice     '[7.5], [0, 5.6, 7.5, 10.0, 13.3]'
    use_se:      !h_tune2_choice     '[0], [0, 1]'
    dropout:     0.0

loss:
  teacher: 'output/r2d2-orig.pt'
  des_loss: 'L1'   # L1 or L2
  des_w: -10.0     # weight for descriptor head loss
  det_w: -0.8      # weight for detector head loss
  qlt_w: -3.0      # weight for quality head loss

optimizer:
  method: 'adam'
  learning_rate: 5e-4
  weight_decay:  !h_tune2_loguniform  '(8e-6, 12e-6), (1e-8, 1e-3)'
  split_params:  0
  eps:           !h_tune2_loguniform  '(1e-8, 1e-7), (1e-9, 1e-6)'

data:
  max_sc:    1.091  # 2**(1/8), half of scale steps used during extraction, i.e. 2**(1/4)
  max_rot:   0.0    # in degrees, synth pair warp related
  max_shear: 0.0    # synth pair warp related
  max_proj:  0.5    # synth pair warp related
  noise_max: !h_tune2_uniform '(0.02, 0.04), (0.00, 0.20)'
  rnd_gain:  !h_tune2_uniform '(1.9, 2.1), (1.0, 3.0)'
  student_rnd_gain: !h_tune2_uniform '(1.15, 1.25), (1.0, 1.5)'
  student_noise_sd: !h_tune2_uniform '(0.01, 0.03), (0.00, 0.10)'
  image_size: 256
  path: 'data/aachen.tar;data/revisitop1m.tar'
  npy: 'false'
  trn_ratio: 0.85
  val_ratio: 0.10
  tst_ratio: 0.05
  workers: 6
