search:
  method: 'asha-bo'
  resume: ''
  metric: 'val_tot_epoch'
  mode: 'max'
  samples: 1
  nodes: 1
  node_types: 'volta'

  # doesnt really work to have different cpu counts as tune.run(..., resources_per_trial={"cpu":...}) needs to be set
  node_type_cpus: '6'
  grace_period: 1500       # for asha only
  reduction_factor: 3   # for asha only
  username: ''
  keyfile: ''
  proxy: ''
  host: ''
  port: 0

training:
  trial: 'aer'
  name: 'v0'
  cache: 'cache'
  output: 'output'
  batch_size: !h_int 8
  batch_mem: 32000
  gpu: 1
  epochs: 24001
  print_freq: 5
  test_freq: 1500   # validation interval in steps
  save_freq: 1
  resume: null
  evaluate: 0
  pretrained: ''
  early_stopping: 0
  reduced_precision: !h_int 0
  auto_lr_find: 0
  deterministic: 1
  accuracy:
    det_mode: 'nms'
    det_kernel_size: 3
    success_px_limit: 5

model:
  arch: !h_str 'disk-def'
  in_channels: !h_int 1
  partial_residual: 0   # not in use
  width_mult:       !h_float  1.0
  pretrained: 0         # not in use
  cache_dir: 'cache'    # not in use
  des_head:
    dimensions:  !h_int 128
    hidden_ch:   !h_int 0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
  det_head:
    after_des:   !h_int 1
    hidden_ch:   !h_int 0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
    act_fn_type: 'R2D2'
  qlt_head:
    skip:        !h_int 0
    after_des:   !h_int 1
    hidden_ch:   !h_int 0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
    act_fn_type: 'R2D2'

loss:
  loss_type: !h_str thresholded
  wpk: !h_float -0.5      # peakiness vs cosim
  wdt: !h_float 0.1      # weight for peakiness and cosim losses
  wap: !h_float 1.0      # weight for expected ap loss
  wqt: !h_float 1.0      # weight for expected ap loss
  det_n: !h_int  24       # detector neighbourhood size
  base: !h_float 0.587      # quality loss target mAP
  nq: 20              # quantizer bins for ap calc
  sampler:
    subq: -8          # cell_d, 	diameter of rectangular cell that is searched for max detection score
    subd: 2           # neg_step, 	negative sampling step
    pos_d: !h_int 7   # pos_r,  	radius from inside which positive samples are gathered
    neg_d: !h_int 16  # neg_min_r, 	min radius for negative samples
    ngh: 2            # neg_max_r=neg_d+ngh,  max radius for negative samples
    border: 32        # border, 	border width, don't sample if closer than this to image borders
    subd_neg: -8      # -, 		not used
    max_neg_b: 8      # max_neg_b, 	get distractors from at most this amount of images in the mini-batch
    maxpool_pos: 1    # -               not used

optimizer:
  method: 'adam'
  learning_rate: !h_float 1e-3
  weight_decay: !h_float 1e-8
  split_params: 0
  eps: !h_float 1e-8

data:
  max_sc:  !h_float 1.091    # 2**(1/8), half of scale steps used during extraction, i.e. 2**(1/4)
  max_rot: !h_float 10.0     # related synth pair warp [deg]
  max_shear: !h_float 0.0    # related to synth pair warp
  max_proj: !h_float 0.50    # related to synth pair warp
  noise_max: !h_float 0.03   # related to all dataset types
  rnd_gain: !h_float 1.2     # related to all dataset types
  image_size: !h_int 224     # related to all dataset types
  path: 'data'
  use_synth: 0
  npy: 'false'
  trn_ratio: 0.90
  val_ratio: 0.10
  tst_ratio: 0.00   # different test set altogether
  workers: 6
