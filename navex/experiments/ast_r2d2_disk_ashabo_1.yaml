search:
  method: 'asha-bo'
  resume: ''
  metric: 'val_tot_epoch'
  mode: 'max'
  samples: 243
  nodes: 6
  node_types: 'volta'   # volta for triton, type_gpu for csc

  # doesnt really work to have different cpu counts as tune.run(..., resources_per_trial={"cpu":...}) needs to be set
  node_type_cpus: '6'
  grace_period: 1500    # for asha only
  reduction_factor: 3   # for asha only
  username: ''
  keyfile: ''
  proxy: ''
  host: ''
  port: 0

training:
  trial: 'ast'
  name: 'ast_hafe_ldisk'
  cache: 'cache'
  output: 'output'
  batch_size: 8
  batch_mem: 32000
  gpu: 1
  epochs: 24001   # actually steps
  print_freq: 5
  test_freq: 1500   # validation interval in steps
  save_freq: 1
  resume: null
  evaluate: 0
  pretrained: ''
  early_stopping: 0
  reduced_precision: 0
  auto_lr_find: 0
  deterministic: 1
  accuracy:
    det_mode: 'nms'
    det_kernel_size: 3
    success_px_limit: 5

model:
  arch: 'disk-def'
  in_channels: 1
  partial_residual: 0   # not in use
  width_mult: 1.0
  pretrained: 0         # not in use
  cache_dir: 'cache'    # not in use
  des_head:
    dimensions:  128
    hidden_ch:   0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
  det_head:
    after_des:   0
    hidden_ch:   0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
    act_fn_type: 'R2D2'
  qlt_head:
    skip:        1
    after_des:   1
    hidden_ch:   0
    exp_coef:    0.0
    use_se:      0
    dropout:     0.0
    act_fn_type: 'R2D2'

loss:
  loss_type: 'disk-p'
  wpk: 0.003  # !h_tune2_loguniform  '(0.9e-3, 1.1e-3), (1.0e-4, 1.0e-2)'  # recycled param: sampling_cost for disk
  wdt: !h_tune2_uniform  '(0.23, 0.27), (0.0, 0.5)'   # recycled param: penalty for disk
  wap: 1.0             # N/A
  wqt: 1.0                                            # recycled param: reward for disk
  det_n: !h_tune2_choice  '[8], [6, 8, 12]'           # detector neighbourhood size, need: (w-2b)/n == (w-2b)//n
  base: !h_tune2_loguniform '(48.0, 52.0), (20.0, 500.0)'  # recycled param: match_theta for disk
  nq: 1500               # recycled param: warmup_batches for disk
  sampler:
    subq: -8             # N/A
    subd: 1              # N/A
    pos_d: !h_tune2_uniform '(1.4, 1.6), (1.0, 5.0)'  # disk max repr err to still consider successful match
    neg_d: 10            # N/A
    ngh: 2               # N/A
    border: 16           #
    subd_neg: -8         # N/A
    max_neg_b:  -1       # matches across these many (n-1) other mini-batch pairs (alias: --max-neg-imgs),
                         #   if -1, det_n determines: {6: 2, 8: 4, 12: 9}
    maxpool_pos: 0       # recycled param: sample_matches, i.e. use ground truth to select features from img2

optimizer:
  method: 'adam'
  learning_rate: 0.001    # for some reason cant use exp form, e.g. 3e-4
  weight_decay:  !h_tune2_loguniform  '(0.9e-6, 1.1e-6), (1e-8, 1e-3)'
  split_params:  0
  eps:           0.00000001   # tried to optimize, doesn't seem to affect much

data:
  max_sc:    1.091  # 2**(1/8), half of scale steps used during extraction, i.e. 2**(1/4)
  max_rot:   !h_tune2_uniform '(8.0, 12.0), (0.0, 20.0)'    # in degrees, synth pair warp related
  max_shear: 0.0    # synth pair warp related
  max_proj:  !h_tune2_uniform '(0.45, 0.55), (0.20, 0.90)'    # synth pair warp related
  noise_max: !h_tune2_uniform '(0.08, 0.12), (0.00, 0.30)'
  rnd_gain:  1.2        # tried to optimize, doesn't seem to affect much
  image_size: 224
  path: 'data/rot-cg67p-osinac.tar;data/cg67p-navcam.tar;data/rot-eros-msi.tar;data/bennu-tagcams.tar;data/rot-synth-bennu-v4.tar;data/rot-itokawa-amica.tar'
  npy: 'false'
  use_synth: !h_tune2_choice  '[0], [0, 1]'
  trn_ratio: 0.85
  val_ratio: 0.15
  tst_ratio: 0.00   # different test set altogether (itokawa)
  workers: 6
